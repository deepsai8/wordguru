{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116f1181-1976-4baf-bfc9-ed786d1676dc",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://github.com/dwyl/english-words/blob/master/words_dictionary.json\n",
    "- https://dumps.wikimedia.org/enwiktionary\n",
    "- https://gcide.gnu.org.ua/download\n",
    "- https://wordnet.princeton.edu/download\n",
    "- http://wordlist.aspell.net/ aka SCOWL (Spell Checker Oriented Word Lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "20de9c4d-73ba-40f9-8a3d-3119f17cf81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import IPython.display\n",
    "\n",
    "import xmltodict\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223d3331-2317-4b4f-a027-5f430f343a8d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### GCIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4ce2d96f-af7f-4f21-b2fc-84543770ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gcide_data(file_path):\n",
    "    # Read XML data\n",
    "    with open(file_path, \"r\") as file:\n",
    "        xml_data = file.read()\n",
    "    \n",
    "    # Wrap with root\n",
    "    wrapped_xml_data = f\"<root>{xml_data}</root>\"\n",
    "    \n",
    "    # Parse ignoring entity errors\n",
    "    parser = etree.XMLParser(recover=True)\n",
    "    tree = etree.fromstring(wrapped_xml_data.encode(), parser=parser)\n",
    "    \n",
    "    # Convert to dictionary if needed\n",
    "    parsed_data = xmltodict.parse(etree.tostring(tree))\n",
    "    \n",
    "    d_out = {}\n",
    "    \n",
    "    for item in parsed_data['root']['p']:\n",
    "        if item is None:\n",
    "            continue\n",
    "        ent = item.get('ent')\n",
    "        if isinstance(ent, list):\n",
    "            ent = ent[0]\n",
    "        if isinstance(ent, dict):\n",
    "            ent = list(ent.keys())[0]\n",
    "        if ent:  # Check if 'ent' exists in the current entry\n",
    "            cleaned_entry = {}\n",
    "            for k, v in item.items():\n",
    "                if k != 'ent':  # Exclude 'ent' itself\n",
    "                    # Handle unhashable types by converting them to strings\n",
    "                    if isinstance(v, (list, dict)):\n",
    "                        # Convert lists/dicts to string\n",
    "                        cleaned_entry[k] = str(v)\n",
    "                    else:\n",
    "                        cleaned_entry[k] = v\n",
    "            ent = str.lower(ent)\n",
    "            d_out[ent] = cleaned_entry\n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7834e9c6-dfab-4a32-8c02-2273d74e108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path\n",
    "\n",
    "def get_filelist(directory_path: str, except_list: Optional[List[str]] = None) -> List[str]:\n",
    "    # Get all files with extensions in the specified directory\n",
    "    files_with_extensions = [f for f in os.listdir(directory_path) if os.path.isfile(os.path.join(directory_path, f)) and '.' in f]\n",
    "    # files_with_extensions = [os.path.abspath(os.path.join(directory_path, f)) for f in os.listdir(directory_path)\n",
    "                             # if os.path.isfile(os.path.join(directory_path, f)) and '.' in f]\n",
    "    if except_list is not None:\n",
    "        fl = set(files_with_extensions) - set(except_list)\n",
    "    else:\n",
    "        fl = files_with_extensions\n",
    "    fl = [os.path.abspath(os.path.join(directory_path, f)) for f in fl]\n",
    "    return fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "86d809d5-b86d-4cfc-b222-45d39ab27492",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"gcide_xml-0.53/\"\n",
    "except_list = ['gcide_authorities.xml', 'gcide_abbreviations.xml', 'gcide.xml']\n",
    "fl = get_filelist(dir_path, except_list)\n",
    "d_gcide = {}\n",
    "for f in fl:\n",
    "    d_gcide.update(read_gcide_data(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b62652b8-e73a-4371-8833-981c75f5c580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108182"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_gcide.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1230b88e-c259-4770-abf7-a7bc795448f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'br': '[None, None]',\n",
       " 'hw': 'R',\n",
       " 'pr': '(r)',\n",
       " 'def': \"{'xex': ['semivowel', 'liquid', 'Guide to Pronunciation'], '#text': 'R, the eighteenth letter of the English alphabet, is a vocal consonant.  It is sometimes called a , and a . See ,  178, 179, and 250-254.'}\",\n",
       " 'xex': 'R',\n",
       " 'rj': \"{'au': 'B. Jonson.'}\",\n",
       " 'source': '1913 Webster',\n",
       " '#text': \".   is the dog's letter and hurreth in the sound.  \\n[]\"}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_gcide['r']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74b590-1e6e-4601-a75a-fad964fdad83",
   "metadata": {},
   "source": [
    "### Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "040f51b1-76dc-45d5-9430-02d78230b569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_wordnet_dict(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"ISO-8859-1\") as file:  # Added encoding to handle potential issues\n",
    "        wordnet_data = file.read()\n",
    "\n",
    "    # Define regex patterns for each part of speech:\n",
    "    # 1. Adjective (a)\n",
    "    adj_pattern = r\"(\\d{8})\\s+\\d{2}\\s+a\\s+\\d{2}\\s+([a-zA-Z_]+)\\s+\\d{1,2}\\s+[\\w=\\s\\+!]*\\s+\\|\\s+(.*)\"\n",
    "    # 2. Adverb (r)\n",
    "    adv_pattern = r\"(\\d{8})\\s+\\d{2}\\s+r\\s+\\d{2}\\s+([a-zA-Z_]+)\\s+\\d{1,2}\\s+[\\w=\\s\\+!]*\\s+\\|\\s+(.*)\"\n",
    "    # 3. Noun (n)\n",
    "    noun_pattern = r\"(\\d{8})\\s+\\d{2}\\s+n\\s+\\d{2}\\s+([a-zA-Z_]+)\\s+\\d{1,2}\\s+[\\w=\\s\\+!]*\\s+\\|\\s+(.*)\"\n",
    "    # 4. Verb (v)\n",
    "    verb_pattern = r\"(\\d{8})\\s+\\d{2}\\s+v\\s+\\d{2}\\s+([a-zA-Z_]+)\\s+\\d{1,2}\\s+[\\w=\\s\\+!]*\\s+\\|\\s+(.*)\"\n",
    "    \n",
    "    wn = {}\n",
    "\n",
    "    # Process the data line by line and match using each pattern\n",
    "    for line in wordnet_data.splitlines():\n",
    "        # Check each pattern one by one\n",
    "        match_adj = re.match(adj_pattern, line)\n",
    "        match_adv = re.match(adv_pattern, line)\n",
    "        match_noun = re.match(noun_pattern, line)\n",
    "        match_verb = re.match(verb_pattern, line)\n",
    "\n",
    "        # If any of the patterns match, process the word and its definition\n",
    "        if match_adj:\n",
    "            word = match_adj.group(2)\n",
    "            definition = match_adj.group(3)\n",
    "            wn[word] = {'def': definition, 'pos': 'adjective', 'source_file': file_path.split('/')[-1]}\n",
    "        elif match_adv:\n",
    "            word = match_adv.group(2)\n",
    "            definition = match_adv.group(3)\n",
    "            wn[word] = {'def': definition, 'pos': 'adverb', 'source_file': file_path.split('/')[-1]}\n",
    "        elif match_noun:\n",
    "            word = match_noun.group(2)\n",
    "            definition = match_noun.group(3)\n",
    "            wn[word] = {'def': definition, 'pos': 'noun', 'source_file': file_path.split('/')[-1]}\n",
    "        elif match_verb:\n",
    "            word = match_verb.group(2)\n",
    "            definition = match_verb.group(3)\n",
    "            wn[word] = {'def': definition, 'pos': 'verb', 'source_file': file_path.split('/')[-1]}\n",
    "\n",
    "    return wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "186174c5-6b8a-4fae-bc79-2683e2eae936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1573"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = \"wordnet\"\n",
    "except_list = ['index.adj', 'index.adv', 'index.noun', 'index.sense', 'index.verb']\n",
    "fl = get_filelist(dir_path, except_list)\n",
    "d_wn = {}\n",
    "for f in fl:\n",
    "    d_wn.update(read_wordnet_dict(f))\n",
    "len(d_wn.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c2ef0-38df-4664-9527-a152238850e1",
   "metadata": {},
   "source": [
    "### SCOWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "02b2f05c-d93b-40a4-ad12-00a8219e1277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_scowl_dict(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        scowl_data = file.read()\n",
    "    words = scowl_data.splitlines()\n",
    "    \n",
    "    # Dictionary to store base form words\n",
    "    base_words_dict = {}\n",
    "    \n",
    "    # Loop over each word in the list\n",
    "    for word in words:\n",
    "        # Remove possessive 's' or any non-alphabetic characters at the end of the word\n",
    "        base_word = re.sub(r\"'s?$|[^a-zA-Z]\", \"\", word)\n",
    "        \n",
    "        # Add the base word to the dictionary with value 1\n",
    "        base_words_dict[base_word] = 1\n",
    "    \n",
    "    # Display the final dictionary\n",
    "    return base_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "637dad1e-40b1-4b0f-b85e-d3fe3599162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"scowl\"\n",
    "except_list = ['README']\n",
    "fl = get_filelist(dir_path)\n",
    "d_scowl = {}\n",
    "for f in fl:\n",
    "    d_scowl.update(read_scowl_dict(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e66b07-8a2d-4aca-8e01-b74daf8a3550",
   "metadata": {},
   "source": [
    "### From excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "69fa35b9-5eaa-495a-a101-a1ef0882fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel/CSV file\n",
    "def read_excel_dict(file_path):\n",
    "    df = pd.read_excel(file_path)  # Use pd.read_csv for CSV files\n",
    "    \n",
    "    # Initialize the dictionary\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Iterate through each row to build the dictionary\n",
    "    for _, row in df.iterrows():\n",
    "        # Ensure the main_word is a string\n",
    "        main_word = str(row['Word']) if pd.notna(row['Word']) else 'Unknown'\n",
    "    \n",
    "        # Handle POS with valid checks for NaN or invalid values\n",
    "        pos = row['POS'] if pd.notna(row['POS']) and row['POS'] != '#N/A' else None\n",
    "    \n",
    "        # Extract synonyms and filter out blanks or NaNs\n",
    "        synonyms = [\n",
    "            row.get(f'Syn0{i}', None) \n",
    "            for i in range(1, 6)\n",
    "            if pd.notna(row.get(f'Syn0{i}', None)) and row[f'Syn0{i}'] != '#N/A'\n",
    "        ]\n",
    "        \n",
    "        # Populate the dictionary\n",
    "        data_dict[main_word] = {\n",
    "            'pos': pos,\n",
    "            'syn': synonyms\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e6d58efc-d0e4-40ce-88e8-bb1759c63fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyWordNet\n",
    "file_path = 'excel/MyWordNet.xlsx' \n",
    "d_mywnet = read_excel_dict(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9338f530-233d-4aac-a0aa-fece5fbf102e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zoo', 'zoological', 'zoologist', 'zoology', 'zoom']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(d_mywordnet.keys())[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7e465202-12ef-4c2d-9216-29d31ad6de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MyWordNet\n",
    "file_path = 'excel/Thesaurus_a-z.xlsx'\n",
    "d_thesaz = read_excel_dict(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c5f9732d-efeb-4ea0-8009-d70612194d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s gravenhage\", \"'tween decks\", '0.22', '.22-calibre', '.22 caliber', '.22 calibre', '.38-caliber', '.38-calibre', '.38 caliber', '.38 calibre', '.45-caliber', '.45-calibre', '.45 caliber', '.45 calibre', '0', '1', '1-dodecanol', '1-hitter', '10', '10-membered', '100', '1000', '10000', '100000', '1000000', '1000000000', '1000000000000', '1000th', '100th', '101', '101st', '105', '105th', '10th', '11', '11-plus', '110', '110th', '115', '115th', '2021-11-11 00:00:00', '11th', '12', '12-tone music', '12-tone system', '120', '120th', '125', '125th', '12th']\n"
     ]
    }
   ],
   "source": [
    "print(list(d_thesaz.keys())[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8e177-deb0-474f-9266-67ff0c95270d",
   "metadata": {},
   "source": [
    "## From englist-words word dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1c7743f0-b6d7-49a6-a0c3-845b4665f3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/deepaksingh/Desktop/experiments/wordguru\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "8db3c09e-514a-455e-8944-2ba5993d7227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw original: 370100\n",
      "\n",
      "words in d_wn: 1573\n",
      "raw after adding d_wn: 370399\n",
      "\n",
      "words in d_scowl: 526912\n",
      "raw after adding d_scowl: 543023\n",
      "\n",
      "words in d_gcide: 108182\n",
      "raw after adding d_gcide: 559056\n",
      "\n",
      "words in d_mywnet: 30259\n",
      "raw after adding d_mywnet: 563831\n",
      "\n",
      "words in d_thesaz: 145789\n",
      "raw after adding d_thesaz: 631016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('english-words/words_dict.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "\n",
    "print(f\"raw original: {len(raw.keys())}\\n\")\n",
    "\n",
    "wordlists = {\n",
    "    \"d_wn\": d_wn,\n",
    "    \"d_scowl\": d_scowl,\n",
    "    \"d_gcide\": d_gcide,\n",
    "    \"d_mywnet\": d_mywordnet,\n",
    "    \"d_thesaz\": d_thesaz,\n",
    "}\n",
    "\n",
    "for name, wl in wordlists.items():\n",
    "    print(f\"words in {name}: {len(wl.keys())}\")\n",
    "    raw.update(wl)\n",
    "    print(f\"raw after adding {name}: {len(raw.keys())}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4f29db80-07e5-4df6-8c9d-c0bd7da1f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_just_words = {k:1 for k,v in raw.items()}\n",
    "with open('raw_words.json', 'w') as f:\n",
    "    json.dump(raw_just_words, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8309676-fb26-4bb9-b0a1-a652a84a01ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
